#!/usr/bin/env python3
"""
ä¿®å¤æ•°æ®æ³„æ¼åçš„æœºå™¨äººå¯¼èˆªæ•°æ®åˆ†æè„šæœ¬
ç§»é™¤file_idç­‰æ³„æ¼ç‰¹å¾,é‡æ–°è®­ç»ƒæ¨¡å‹
"""

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

import matplotlib
matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report, roc_curve, roc_auc_score
)
import joblib

# è®¾ç½®éšæœºç§å­
np.random.seed(42)

plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

print("=" * 80)
print("å®¤å†…æœºå™¨äººå¯¼èˆªæ•°æ®åˆ†æ - ä¿®å¤ç‰ˆ")
print("=" * 80)

# 1. åŠ è½½æ•°æ®
print("\n[1/8] åŠ è½½æ•°æ®...")
df = pd.read_csv('../data/sensor_readings_2.csv')
print(f"âœ… æ•°æ®å½¢çŠ¶: {df.shape}")

# 2. æ•°æ®æ¸…æ´—
print("\n[2/8] æ•°æ®æ¸…æ´—...")
df_clean = df.copy()

# æ ‡ç­¾ç¼–ç 
le = LabelEncoder()
df_clean['surface_encoded'] = le.fit_transform(df_clean['surface'])
print(f"âœ… ç›®æ ‡ç±»åˆ«: {list(le.classes_)}")
print(f"   ç±»åˆ«åˆ†å¸ƒ: {df_clean['surface'].value_counts().to_dict()}")

# 3. ç‰¹å¾é€‰æ‹© - ç§»é™¤æ•°æ®æ³„æ¼ç‰¹å¾
print("\n[3/8] ç‰¹å¾å·¥ç¨‹ - ç§»é™¤æ•°æ®æ³„æ¼ç‰¹å¾...")

leakage_features = ['file_id', 'direction', 'horn']
print("âš ï¸  ç§»é™¤ä»¥ä¸‹æ³„æ¼ç‰¹å¾:")
for feat in leakage_features:
    if feat in df_clean.columns:
        print(f"   - {feat}")

feature_cols = [col for col in df_clean.columns
               if col not in ['surface', 'surface_encoded'] + leakage_features
               and df_clean[col].dtype in [np.int64, np.float64]]

X = df_clean[feature_cols].values
y = df_clean['surface_encoded'].values

print(f"âœ… ç‰¹å¾æ•°é‡: {len(feature_cols)}")
print(f"   ç‰¹å¾åˆ—è¡¨: {feature_cols[:5]}...")

# 4. æ ‡å‡†åŒ–
print("\n[4/8] æ•°æ®æ ‡å‡†åŒ–...")
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
print("âœ… æ ‡å‡†åŒ–å®Œæˆ")

# 5. åˆ’åˆ†æ•°æ®é›†
print("\n[5/8] åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†...")
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)
print(f"âœ… è®­ç»ƒé›†: {X_train.shape}, æµ‹è¯•é›†: {X_test.shape}")

# 6. è®­ç»ƒæ¨¡å‹
print("\n[6/8] è®­ç»ƒæ¨¡å‹...")
results = []

# é€»è¾‘å›å½’
print("  [1/3] é€»è¾‘å›å½’...")
lr_model = LogisticRegression(max_iter=1000, random_state=42)
lr_model.fit(X_train, y_train)
y_pred_lr = lr_model.predict(X_test)
y_pred_proba_lr = lr_model.predict_proba(X_test)
cv_scores_lr = cross_val_score(lr_model, X_train, y_train, cv=5, scoring='accuracy')

results.append({
    'æ¨¡å‹': 'é€»è¾‘å›å½’',
    'å‡†ç¡®ç‡': accuracy_score(y_test, y_pred_lr),
    'ç²¾ç¡®ç‡': precision_score(y_test, y_pred_lr, average='weighted'),
    'å¬å›ç‡': recall_score(y_test, y_pred_lr, average='weighted'),
    'F1åˆ†æ•°': f1_score(y_test, y_pred_lr, average='weighted'),
    'äº¤å‰éªŒè¯': cv_scores_lr.mean()
})

print(f"      å‡†ç¡®ç‡: {results[-1]['å‡†ç¡®ç‡']:.4f}")
print(f"      F1åˆ†æ•°: {results[-1]['F1åˆ†æ•°']:.4f}")
print(f"      äº¤å‰éªŒè¯: {results[-1]['äº¤å‰éªŒè¯']:.4f}")

# éšæœºæ£®æ—
print("  [2/3] éšæœºæ£®æ—...")
rf_model = RandomForestClassifier(n_estimators=200, max_depth=20,
                                 min_samples_split=5, random_state=42, n_jobs=-1)
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)
y_pred_proba_rf = rf_model.predict_proba(X_test)
cv_scores_rf = cross_val_score(rf_model, X_train, y_train, cv=5, scoring='accuracy')

results.append({
    'æ¨¡å‹': 'éšæœºæ£®æ—',
    'å‡†ç¡®ç‡': accuracy_score(y_test, y_pred_rf),
    'ç²¾ç¡®ç‡': precision_score(y_test, y_pred_rf, average='weighted'),
    'å¬å›ç‡': recall_score(y_test, y_pred_rf, average='weighted'),
    'F1åˆ†æ•°': f1_score(y_test, y_pred_rf, average='weighted'),
    'äº¤å‰éªŒè¯': cv_scores_rf.mean()
})

print(f"      å‡†ç¡®ç‡: {results[-1]['å‡†ç¡®ç‡']:.4f}")
print(f"      F1åˆ†æ•°: {results[-1]['F1åˆ†æ•°']:.4f}")
print(f"      äº¤å‰éªŒè¯: {results[-1]['äº¤å‰éªŒè¯']:.4f}")

# SVM
print("  [3/3] SVM...")
svm_model = SVC(kernel='rbf', C=1.0, gamma='scale', probability=True, random_state=42)
svm_model.fit(X_train, y_train)
y_pred_svm = svm_model.predict(X_test)
y_pred_proba_svm = svm_model.predict_proba(X_test)
cv_scores_svm = cross_val_score(svm_model, X_train, y_train, cv=5, scoring='accuracy')

results.append({
    'æ¨¡å‹': 'SVM',
    'å‡†ç¡®ç‡': accuracy_score(y_test, y_pred_svm),
    'ç²¾ç¡®ç‡': precision_score(y_test, y_pred_svm, average='weighted'),
    'å¬å›ç‡': recall_score(y_test, y_pred_svm, average='weighted'),
    'F1åˆ†æ•°': f1_score(y_test, y_pred_svm, average='weighted'),
    'äº¤å‰éªŒè¯': cv_scores_svm.mean()
})

print(f"      å‡†ç¡®ç‡: {results[-1]['å‡†ç¡®ç‡']:.4f}")
print(f"      F1åˆ†æ•°: {results[-1]['F1åˆ†æ•°']:.4f}")
print(f"      äº¤å‰éªŒè¯: {results[-1]['äº¤å‰éªŒè¯']:.4f}")

# 7. ç”Ÿæˆå›¾è¡¨
print("\n[7/8] ç”Ÿæˆåˆ†æå›¾è¡¨...")

# æ¨¡å‹å¯¹æ¯”å›¾
results_df = pd.DataFrame(results)
fig, ax = plt.subplots(figsize=(12, 6))
x = np.arange(4)
width = 0.25
metrics = ['å‡†ç¡®ç‡', 'ç²¾ç¡®ç‡', 'å¬å›ç‡', 'F1åˆ†æ•°']
ax.bar(x - width, results_df.iloc[0][1:5], width, label='é€»è¾‘å›å½’', alpha=0.8)
ax.bar(x, results_df.iloc[1][1:5], width, label='éšæœºæ£®æ—', alpha=0.8)
ax.bar(x + width, results_df.iloc[2][1:5], width, label='SVM', alpha=0.8)
ax.set_xlabel('Evaluation Metrics', fontsize=12)
ax.set_ylabel('Score', fontsize=12)
ax.set_title('Model Performance Comparison (Fixed)', fontsize=14)
ax.set_xticks(x)
ax.set_xticklabels(metrics)
ax.legend()
ax.grid(True, alpha=0.3, axis='y')
ax.set_ylim([0.5, 1.0])
plt.tight_layout()
plt.savefig('../figures/08_model_comparison_fixed.png', dpi=300, bbox_inches='tight')
print("  âœ… 08_model_comparison_fixed.png")

# æ··æ·†çŸ©é˜µ
fig, axes = plt.subplots(1, 3, figsize=(18, 5))
models = [
    ('Logistic Regression', y_pred_lr),
    ('Random Forest', y_pred_rf),
    ('SVM', y_pred_svm)
]
for idx, (name, y_pred) in enumerate(models):
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
               xticklabels=le.classes_, yticklabels=le.classes_,
               ax=axes[idx])
    axes[idx].set_title(f'{name}\nConfusion Matrix', fontsize=12)
    axes[idx].set_ylabel('True Label')
    axes[idx].set_xlabel('Predicted Label')
plt.tight_layout()
plt.savefig('../figures/09_confusion_matrices_fixed.png', dpi=300, bbox_inches='tight')
print("  âœ… 09_confusion_matrices_fixed.png")

# ROCæ›²çº¿
plt.figure(figsize=(10, 8))
models_proba = [
    ('Logistic Regression', y_pred_proba_lr[:, 1]),
    ('Random Forest', y_pred_proba_rf[:, 1]),
    ('SVM', y_pred_proba_svm[:, 1])
]
for name, y_score in models_proba:
    fpr, tpr, _ = roc_curve(y_test, y_score)
    auc_score = roc_auc_score(y_test, y_score)
    plt.plot(fpr, tpr, label=f'{name} (AUC = {auc_score:.3f})', linewidth=2)
plt.plot([0, 1], [0, 1], 'k--', label='Random Guess', linewidth=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate', fontsize=12)
plt.ylabel('True Positive Rate', fontsize=12)
plt.title('ROC Curves (Fixed)', fontsize=14)
plt.legend(loc="lower right", fontsize=10)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('../figures/10_roc_curves_fixed.png', dpi=300, bbox_inches='tight')
print("  âœ… 10_roc_curves_fixed.png")

# ç‰¹å¾é‡è¦æ€§
feature_importance = pd.DataFrame({
    'Feature': feature_cols,
    'Importance': rf_model.feature_importances_
}).sort_values('Importance', ascending=False)

plt.figure(figsize=(10, 10))
top_n = min(20, len(feature_importance))
top_features = feature_importance.head(top_n)
plt.barh(range(len(top_features)), top_features['Importance'], color='steelblue', alpha=0.8)
plt.yticks(range(len(top_features)), top_features['Feature'])
plt.xlabel('Importance Score', fontsize=12)
plt.title(f'Top {top_n} Feature Importance (Random Forest)', fontsize=14)
plt.gca().invert_yaxis()
plt.grid(True, alpha=0.3, axis='x')
plt.tight_layout()
plt.savefig('../figures/11_feature_importance_fixed.png', dpi=300, bbox_inches='tight')
print("  âœ… 11_feature_importance_fixed.png")

# 8. ä¿å­˜æ¨¡å‹å’Œç»“æœ
print("\n[8/8] ä¿å­˜æ¨¡å‹å’Œç»“æœ...")
joblib.dump(rf_model, '../code/best_model_fixed.pkl')
joblib.dump(scaler, '../code/scaler_fixed.pkl')
joblib.dump(le, '../code/label_encoder_fixed.pkl')
print("  âœ… best_model_fixed.pkl")
print("  âœ… scaler_fixed.pkl")
print("  âœ… label_encoder_fixed.pkl")

results_df.to_csv('../report/model_comparison_fixed.csv', index=False, encoding='utf-8-sig')
feature_importance.to_csv('../report/feature_importance_fixed.csv', index=False, encoding='utf-8-sig')
print("  âœ… model_comparison_fixed.csv")
print("  âœ… feature_importance_fixed.csv")

# æœ€ç»ˆæ€»ç»“
print("\n" + "=" * 80)
print("ğŸ‰ åˆ†æå®Œæˆ!")
print("=" * 80)
print("\nğŸ“Š æ¨¡å‹æ€§èƒ½å¯¹æ¯”:")
print(results_df.to_string(index=False))

print(f"\nğŸ”‘ Top 5 é‡è¦ç‰¹å¾:")
for idx, row in feature_importance.head(5).iterrows():
    print(f"  {idx+1}. {row['Feature']}: {row['Importance']:.4f}")

print("\nâœ… æ‰€æœ‰å›¾è¡¨å·²ä¿å­˜åˆ° ../figures/ ç›®å½•")
print("âœ… æ‰€æœ‰æ¨¡å‹å·²ä¿å­˜åˆ° ../code/ ç›®å½•")
print("=" * 80)
